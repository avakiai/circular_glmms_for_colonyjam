---
title: "Circular GLM"
output:
  html_document:
    df_print: paged
---

Circular Generalized Linear Model fit to our data.

First, let's split the data into a training and test set 
```{r Set up cross-val}
library(tidyverse)
library(bpnreg)
library(caret)
set.seed(42)

calls <- read.csv("sample_data.csv") %>%
  mutate(max.per = 1/modulation, .before = "start.per") %>% 
  # convert period of call onsets into phases between 0 and 2pi
  mutate(start.phase = (start.per/max.per)*(2*pi), .before = "start.per",
         modulation = factor(modulation))

glm.calls <- calls %>% mutate(phase.circ = circular(start.phase, 
                                                 units = "radians", 
                                                 zero = 0, 
                                                 modulo = "asis", #?
                                                 rotation = "counter"), .after = start.phase) %>%
          dplyr::select(c(phase.circ, start.phase, start.per, condition, session, modulation, group)) %>%
                            # quick and dirty version of dummy coding
                            mutate(condition = as.factor(as.numeric(as.factor(condition))-1), 
                                   modulation = as.factor(as.numeric(as.factor(modulation))-1),
                                   group = as.numeric(group))

head(glm.calls)

saveRDS(object = glm.calls, 'glmdata')
glm.calls <- readRDS('glmdata')

# split data
training.samples <- glm.calls$modulation %>% caret::createDataPartition(p = 0.8, list = FALSE) 
train.data  <- glm.calls[training.samples, ] 
test.data <- glm.calls[-training.samples, ]
saveRDS(object = train.data, 'trainingdata')

tibble(
  cbind(train.data %>% group_by(modulation, condition) %>% summarise(n_train = length(phase.circ)),
        test.data %>% group_by(modulation, condition) %>% summarise(n_test = length(phase.circ)) 
        %>% ungroup() %>% dplyr::select(n_test)))


```

Okay, next we're going to try to model the circular data using either the wrapped 
or a projected normal general linear model.


##### Bayesian Models
Build-up procedure:

```{r}
# example 1
fit.Motor <- bpnr(pred.I = Phaserad ~ 1 + Cond, data = Motor,
                  its = 100, burn = 10, n.lag = 3)
  fit.Motor
  
  # try this
  predict(fit.Motor)

# compare r and mm

fitne <- bpnme(Error.rad ~ Maze + Trial.type + (1|Subject), Maps, its = 100)

fitr <- bpnr(Error.rad ~ Maze + Trial.type, Maps, its = 100)



  coef_circ(fitne, type = "categorical", units = "radians")

  traceplot(fitne, parameter = "beta1")

  BFc(fitne, hypothesis = "Trial.type1 < Maze1")

```
Questions: 
1. What does the coef output tell us? Can we use it? Does it tell us anything more or more useful than the frequentist version?
2. What is the Bayes factor? How do we test Hs that silence != masks
3. How to visualize?
4. How to get and compare predictions?

```{r}
mm0 <- bpnme(phase.circ ~ (1|group), 
             data = train.data, its = 10000, burn = 100, n.lag = 3, seed = 42)

mm1 <- bpnme(phase.circ ~ conditon + (1|group), 
             data = train.data, its = 10000, burn = 100, n.lag = 3, seed = 42)

mm2 <- bpnme(phase.circ ~ condition + modulation + (1|group), 
             data = train.data[train.data$modulation==0,], its = 100, burn = 100, n.lag = 3, seed = 42)

mm2prime <- bpnme(phase.circ ~ condition + modulation + (1|group), 
             data = train.data, its = 10000, burn = 100, n.lag = 3, seed = 42)
```

# summary statistics (mean, mode, standard deviation and 95% highest posterior density interval) of the posterior samples of circular coefficients
```{r}
m1 <- bpnr(phase.circ ~ condition,
             data = train.data)
  #m1
  #coef_circ(m1, type = "categorical", units = "radians")

m2 <- bpnr(phase.circ ~ condition + modulation,
             data = train.data)
  #m2 
  #coef_circ(m2, type = "categorical", units = "radians")

  #traceplot(m2, parameter = "beta1")
  
  # BFs
  # BFc(m2, hypothesis = "`conditionfull mask` < `conditionhalf mask`")


m2prime <- bpnr(phase.circ ~ condition * modulation,
             data = train.data)
  #m2prime
  
  #coef_circ(m2prime, type = "categorical", units = "radians")

  #BFc(m2prime, hypothesis = "modulation1 < condition2")


fit(m1)
fit(m2)
fit(m2prime)

m1.8 <- bpnr(phase.circ ~ condition,
             data = train.data[train.data$modulation==0,], its = 10000, burn = 750, n.lag = 3, seed = 42)
  m1.8
  coef_circ(m1.8, type = "categorical", units = "radians")

  BFc(m1.8, hypothesis = "condition2 < condition1") # half is more similar to silence than full is 


  
m1.15 <- bpnr(phase.circ ~ condition,
             data = train.data[train.data$modulation==1,], its = 10000, burn = 750, n.lag = 3, seed = 42)
  m1.15
  coef_circ(m1.15, type = "categorical", units = "radians")
  
  
  
  
  
  
```
If predictors are continuous:
* `ax` = the location of the inflection point of the regression curve on the axis of the predictor.
* `ac` = the location of the inflection point of the regression curve on the axis of the circular outcome.
* `bc` = the slope of the tangent line at the inflection point. An increase of 1 unit of the predictor at the inflection point leads to a `bc` change in the circular outcome.
* `AS` = the average slopes of the circular regression. An increase of 1 unit of the predictor leads to a `AS` change in the circular outcome on average.
* `SAM` = the circular regression slopes at the mean.An increase of 1 unit of the predictor leads to a `SAM` change in the circular outcome at the average predictor value.
* `SSDO`= the signed shortest distance to the origin.

If predictors are categorical:
The output returns summary statistics for the posterior distributions of the circular means for all categories and combination of categories of the categorical variables in the model, as well as differences between these means. 


Fit statistics:
All five fit statistics are computed as in Gelman et.al. (2014). The `lppd` is
an estimate of the expected log predictive density, the `DIC` is the Deviance
Information Criterion, the `DIC_alt` is a version of the DIC that uses a
slightly different definition of the effective number of parameters, the `WAIC1`
and `WAIC2` are the two versions of the Watanabe-Akaike or Widely Available
Information Criterion presented in Gelman et.al. (2014).

### How to obtain the raw posterior estimates:

Raw posterior estimates are stored in the following objects:

* `a.x` = posterior samples for the the locations of the inflection point of the regression curve on the axis of the predictor.
* `a.c` = posterior samples for the the locations of the inflection point of the regression curve on the axis of the circular outcome.
* `b.c` = posterior samples for the slopes of the tangent line at the inflection point. 
* `AS` = posterior samples for the average slopes of the circular regression.
* `SAM` = posterior samples for the circular regression slopes at the mean.
* `SSDO`= posterior samples for the signed shortest distance to the origin.
* `circ.diff` = posterior samples for the circular differences between intercept and other categories of categorical variables.
* `beta1` = posterior samples for the fixed effects coefficients for the first component.
* `beta2` = posterior samples for the fixed effects coefficients for the second component.

In circular mixed-effects models the following additional parameters can be obtained:

* `b1` = posterior samples for the random effects coefficients for the first component.
* `b2` = posterior samples for the random effects coefficients for the second component.
* `circular.ri` = posterior samples for the circular random intercepts for each individual.
* `omega1` = posterior samples for the random effect variances of the first component.
* `omega2` = posterior samples for the random effect variances of the first component.
* `cRS` = posterior samples for the circular random slope variance.
* `cRI` = posterior samples of the mean resultant length of the circular random intercept, a measure of concentration. 


## Issues

- Means don't seem to be in range 0-2pi
- Feature request: embed variances into model output
Clarifications:
- Confirm: cannot get regression curve parameters without a continuous predictor? Is there a structural reason for this? 
- Prediction: extract predictions?
- Plot: with categorial predictors, any outcome we can visualize other than predicted means?

